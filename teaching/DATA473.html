<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<title>DATA473 Algorithms in Data Science - Alex Gavryushkin</title>
	<meta name="author" content="Alex Gavryushkin">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
	<link rel="stylesheet" href="../talks/revealjs/dist/reveal.css">
	<link rel="stylesheet" href="../talks/revealjs_themes_biods/simple_gavruskin.css" id="theme">
	<link rel="stylesheet" href="../talks/revealjs/plugin/highlight/zenburn.css">
	<link rel="icon" href="../favicon.ico" />
</head>

<body>
<div class="reveal">
<div class="slides">

<section>
	<h4>
	  DATA473
	</h4>
	<h3>
	  Algorithms in Data Science
	</h3>
	<h4>
	  <a href="https://biods.org/alex/">Alex Gavryushkin</a>
	</h4>
	<h5>
	  <a href="https://biods.org/people/">
	    <img data-src="../talks/images/bioDS_lab_UC_logo.png" width=20%>
	  </a>
	</h5>
	<font size="5">
	  2023 Semester 2<br>
	</font>
</section>

<section>
  <section>
	<h3>Table of contents</h3>

	<ul>
	  <li> <a href="#/P0">Part 0: Introduction</a>
		<ul>
		  <li> <a href="#/L1">Lecture 1: Background</a>
		  <li> <a href="#/L2">Lecture 2: Turing machines</a>
		  <li> <a href="#/L3">Lecture 3: Universal Turing machine</a>
		</ul>
	  <li> <a href="#/P1">Part 1: Classical approaches to scalability</a>
		<ul>
		  <li> <a href="#/L4">Lecture 4: Recursive functions, halting problem</a>
		  <li> <a href="#/L5">Lecture 5: Kolmogorov complexity</a>
		  <li> <a href="#/L6">Lecture 6: Properties of Kolmogorov complexity</a>
		  <li> <a href="#/L7">Lecture 7: Basic algorithms recap</a>
		  <li> <a href="#/L8">Lecture 8: Sorting</a>
		</ul>
	  <!--
		<li> <a href="#/P2">Part 2: Scalability in statistical learning</a>
		<li> <a href="#/P3">Part 3: Scalability in modern machine learning</a>
	  -->
	</ul>
  </section>

  <section>
    <font size="6">
	<h3>Literature, resources</h3>
	  <ul>
	    <li> <a href="https://youtube.com">YouTube</a>
		<ul>
		  <li><a href="https://youtu.be/XfpMkf4rD6E?t=614">Introduction to transformers</a> <a href="#/P#">#P3</a>
		</ul>
	    <li> <a href="COSC341">These slides</a>
	    <li> <a href="https://docs.google.com/document/d/1YkzpoqHQvIJTTDRF9bGhYUt43vKXMnYz95n3hyLkw1Q/edit">Complementary (rapidly evolving) lecture notes</a>
	    <li> <a href="https://timroughgarden.org/">Tim Roughgarden</a>: <a href="http://www.algorithmsilluminated.org/">Algorithms Illuminated</a>
	    <li> <a href="https://paperpile.com/shared/BqtRx0">Papers</a> including these that we will use:
		<ul>
		  <li> <a href="https://paperpile.com/app/p/0abca75d-205e-0381-9b08-c59a61ad3029">Siegelmann, On the computational power of neural nets, 1992</a>
		  <li> <a href="https://paperpile.com/app/p/fd6708ab-063f-0dde-b115-da0372a87c12">Kaiser, Neural GPUs Learn Algorithms, 2015
		  <li> <a href="https://paperpile.com/app/p/8722d1ce-240b-0b04-a7a2-6c040c1d8c31">Zhou, Universality of deep convolutional neural networks, 2020</a>
		  <li> <a href="https://paperpile.com/app/p/2f91ef4f-dc9e-097a-912e-4b11c3257d74">Pérez, Attention is Turing complete, 2021</a>
		  <li> <a href="https://paperpile.com/app/p/1b25b93d-5ced-0e44-b477-965813bf4561">Mankowitz, Faster Sorting Algorithms, 2023</a>
		</ul>
	  </ul>
    </font>
  </section>

  <section>
	<h3><a href="https://biods.org/people/">Biological Data Science Lab @UCNZ</a></h3>
	<img data-src="../assets/2023-02-17-bioDS_lab.jpg" width=75%><br>
	GitHub: <a href="https://github.com/bioDS">@bioDS</a><br>
	Twitter: <a href="https://twitter.com/bioDS_lab">@bioDS_lab</a>
  </section>
</section>

<section id="P0">
	<h3>Part 0: Introduction</h3>
</section>

<section>
	<img class="stretch" data-src="images/DS_diagramme.svg"><br>
	<p style="font-size:13pt">
	  ML = Machine Learning<br>
	  CoSt = Computational Statistics<br>
	  AS = Applied Statistics
	</p>
</section>

<section>
	<img class="stretch" data-src="images/why_ToC.svg"><br>
	<p style="font-size:11pt">
	  Garey and Johnson. <i>Computers and Intractability.</i> 1979
	</p>
</section>

<section>
	<h3>
	  Why biological data science?
	</h3>
	<p class="fragment" data-fragment-index="1">
	  Because the skills are highly transferable:
	  <ul>
	    <li class="fragment" data-fragment-index="1"> Scale
	    <li class="fragment" data-fragment-index="2"> Heterogeneity
	    <li class="fragment" data-fragment-index="3"> Visualisation and communication
	    <li class="fragment" data-fragment-index="4"> High-performance computing
	  <ul>
	</p>
</section>

<section>
	<h3>
	  Why is it suddenly a thing?
	</h3>
	<img class="stretch" data-src="images/costpergenome_2017.jpg">
</section>

<section id="L1">
	<h2>Lecture 1: Background</h2>
</section>

<section>
  <section>
	<h3>Finite state automata</h3>
	<img data-src="images/FSA1.png">
  </section>

  <section>
	<img data-src="images/FSA2.png">
  </section>

  <section>
	<img data-src="images/FSA3.png">
  </section>
</section>

<section>
	<h3>DFA = NFA</h3>
	<h3>Pumping Lemma</h3>
</section>

<section id="L2">
	<h2>Lecture 2: Turing machines</h2>
</section>

<section>
	A <b>Turing machine</b>, $M$, consists of:
	<ul>
	  <li> A finite set $\Gamma$ called the <b>alphabet</b> (or tape symbols)
	  <li> A finite set $Q$ of <b>states</b>
	  <li> A distinguished state $q_0 \in Q$ called the <b>starting state</b>
	  <li> A subset $F \subseteq Q$ of <b>halting states</b>
	  <li> A distinguished symbol $\beta \in \Gamma$ called the <b>blank symbol</b> (or the empty cell)
	  <li> A partial function $F: (Q \setminus F) \times \Gamma \to Q \times \Gamma \times \{L, R, \varnothing\}$<br>
	    called the <b>programme</b>
	</ul>
</section>

<section>
	The programme of a Turing machine $M$ <b>terminates</b> if $M$ reaches a state $q \in F$.<br>
	The program does not terminate otherwise.

	<p class="fragment" data-fragment-index="1">
	  Hence a non-terminating programme either <i>hangs</i> (infinite loop) or <i>crashes</i> (reaches a state and tape symbol with no command to execute).
	</p>

	<p style="color:red" class="fragment" data-fragment-index="2">
	  We will be starting Turing machines on
	  $$
	    \beta\underset{\Delta}{a_1} a_2 \ldots a_n \beta
	  $$
	</p>
</section>

<section>
	$$
	  \begin{align}
	    \mbox{Example 1} &\\
	    & q_0 \beta \to \mathrm{accept} \beta
	  \end{align}
	$$

	<p class="fragment" data-fragment-index="1">
	$$
	  \begin{align}
	    \mbox{Example 2} &\\
	    & q_0 \beta \to \mathrm{accept} \beta\\
	    & q_0 0 \to q_0 \beta R\\
	    & q_0 1 \to q_0 \beta R
	  \end{align}
	$$

	<p class="fragment" data-fragment-index="2">
	$$
	  \begin{align}
	    \mbox{Example 3} &\\
	    & q_0 \beta \to \mathrm{accept} \beta\\
	    & q_0 0 \to q_0 1 R\\
	    & q_0 1 \to q_0 0 R
	  \end{align}
	$$
	</p>
</section>

<section>
	A Turing Machine $M$ <b>computes a function</b> $f: \mathbb N \to \mathbb N$ if<br>
	for all $x$ and $y$:
	$$
	  f(x) = y
	$$
	$$
	  \Updownarrow
	$$
	$$
	  \beta \underbrace{1 \ldots 1}_{x \mbox{ times}} \beta \Rightarrow_M \beta \underbrace{1 \ldots 1}_{y \mbox{ times}} \beta \mbox{ and $M$ halts}
	$$

	<p class="fragment" data-fragment-index="1">
	  We denote the latter by $M(x) = y$.<br>
	  We write $M(x)\downarrow$ for $M$ halts on $\beta \underbrace{1 \ldots 1}_{x \mbox{ times}} \beta$
	</p>
</section>

<section>
	A Turing Machine $M$ <b>computes a function</b> $f: \mathbb N^2 \to \mathbb N$ if<br>
	for all $x_1, x_2$, and $y$:
	$$
	  f(x_1, x_2) = y
	$$
	$$
	  \Updownarrow
	$$
	$$
	  \beta \underbrace{1 \ldots 1}_{x_1 \mbox{ times}} 0 \underbrace{1 \ldots 1}_{x_2 \mbox{ times}} \beta \Rightarrow_M \beta \underbrace{1 \ldots 1}_{y \mbox{ times}} \beta \mbox{ and $M$ halts}
	$$
	Similarly for more than two variables $x_1, x_2, \ldots, x_n$.
</section>

<section>
	<b>Example 1:</b> compute the following functions.
	$$
	  \begin{align}
	    & f(x) = 0\\
	    & g(x) = x + 1\\
	    & h(x, y) = x + y
	  \end{align}
	$$

	<p class="fragment" data-fragment-index="1">
	  <b>Example 2:</b> implement the following module COPY.
	    $$
	      \beta \underbrace{1 \ldots 1}_{x \mbox{ times}} \beta \Rightarrow_M \beta \underbrace{1 \ldots 1}_{x \mbox{ times}} 0 \underbrace{1 \ldots 1}_{x \mbox{ times}} \beta
	    $$
	</p>

	<p class="fragment" data-fragment-index="2">
	  <b>Example 3:</b> compute:
	    $$
	      f(x, y) = xy
	    $$
	</p>
</section>

<section>
	<b>Example 4:</b> recognise the following languages.
	<p>
	  $a^* b^*$
	</p>
	<p class="fragment" data-fragment-index="1">
	  $\{a^n b^n \mid n \in \mathbb N\}$
	</p>
</section>

<section>
	<h3>Church–Turing thesis</h3>
	A problem can be solved by an algorithm if and only if it can be solved by a Turing machine.
</section>

<section>
	<h3>Universal Turing Machine</h3>
	<b>Theorem:</b> There exists a Turing Machine $U$ which computes all computable functions.<br>
	That is, for every computable function $f(x)$ there exists a number $s$ such that $U(s, x) = y$ if and only if $f(x) = y$.
	<img class="fragment" data-fragment-index="1" data-src="images/hooray.svg">
</section>

<section>
	<h3>Turing machine</h3>

	<iframe width="560" height="315" src="https://www.youtube.com/embed/E3keLeMwfHY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>

<section id="L3">
	<h2>Lecture 3: Universal Turing machine</h2>
</section>

<section>
	<h3>
	  <b>Theorem:</b> There exists a universal Turing Machine $U$.<br>
	</h3>
	That is, for every Turing Machine $M(\bar x)$ there exists a number $s$ such that $U(s, \bar x) = y$ if and only if $M(\bar x) = y$.
	<p class="fragment" data-fragment-index="1">
	  <i><u>Proof</u></i><br>
	  Idea 1: $s$ is $M$'s "number"</br>
	  Idea 2: $U$ "decodes" $s$ into $M$'s program</br>
	  Idea 3: $U$ "simulates" $M$'s execution on $\bar x$
	</p>
</section>

<section>
	<h3>Trick 1: Recursive functions</h3>
	<h3>Trick 2: Multi-tape Turing machines</h3>
</section>

<section id="L4">
	<h2>Lecture 4: Recursive functions, halting problem</h2>
</section>

<section>
	<h3>Primitive recursive functions</h3>
</section>

<section>
	<h3>Partial recursive functions</h3>
</section>

<section>
	<h3>Recursive function that is not primitive recursive</h3>
</section>

<section>
	<h3>Halting problem</h3>
	<b>Theorem:</b> Not everything is computable.<br>
	For example, the following function is not computable
	$$
	  h(x) = \begin{cases}
		  U(x, x) + 1,	& \mbox{ if } U(x, x) \downarrow\\
		  0,		& \mbox{ otherwise}
		\end{cases}
	$$
	<img class="fragment" data-fragment-index="1" data-src="images/oh_no.svg" width="25%">
</section>

<section id="L5">
	<h2>Lecture 5: Kolmogorov complexity</h2>
</section>

<section>
        <img data-src="images/data473_summer_projects_ad.svg"><br>
	<a href="https://learn.canterbury.ac.nz/course/view.php?id=19467">https://learn.canterbury.ac.nz/course/view.php?id=19467</a>
</section>

<section>
	<h3>Definition</h3>
	<b>Kolmogorov complexity</b> $K(\bar x)$ of string $\bar x$ is $$\min {\{|M_s| ~\bigg |~ U(s, 0) = \bar x\}}$$ where $U$ is a universal Turing machine.<br><br>

	<h3 class="fragment" data-fragment-index="1">Theorem</h3>
	<p class="fragment" data-fragment-index="1">
	  The definition is invariant under the programming language, that is,
	  $K_U(\bar x) \leq K_A (\bar x) + c_A$ for every algorithm $A$.

	</p>
</section>

<section id="dfn_good_compress">
	<a href="https://www.youtube.com/live/AKMuA_TVz3A?si=zZsN0Alflgigbled&t=870">Connection of KC to modern LLMs</a> by Ilya Sutskever (OpenAI).<br><br>

	Given two languages $A$ and $B$ and a compression algorithm $M$, we say that $M$ is <i>good</i> if $$|M(A, B)| \leq |M(A)| + |M(B)| + c$$

	<p class="fragment" data-fragment-index="1">
	  With the idea of (unsupervised) model pre-training in mind, what is <i>the best</i> compression algorithm?
	</p>
</section>

<section>
	<h3>Kolmogorov complexity as the ultimate goal for compressors</h3>
	And neural network training is effectively a search for such compressor.
</section>

<section id="L6">
	<h2>Lecture 6: Properties of Kolmogorov complexity</h2>
</section>

<section>
	<h3>Definition</h3>
	<b>Kolmogorov complexity</b> $K(x)$ of a string $x$ is $$\min {\{|M_s| ~\bigg |~ U(s, 0) = x\}}$$ where $U$ is a universal Turing machine.<br><br>
</section>

<section>
	<h3>Theorem</h3>
	The following problems cannot be solved by an algorithm:
	  <ol>
		<li> Algorithm triviality
		<li> Algorithm equivalence
		<li> Kolmogorov complexity
	  </ol>
</section>

<section>
  <section>
	<h3>Theorem</h3>
	Properties of $K(x)$:
	  <ol>
		<li> For all $x, K(x) \leq |x| + c$
		<li> $K(xx) = K(x) + c$
		<li> $K(1^n) \leq O(\log n)$
		<li> $K(1^{n!}) \leq O(\log n)$
		<li> $K(xy) \leq K(x) + K(y) + O(\log(\min\{K(x), K(y)\}))$ (compare with the definition of <a href="#/dfn_good_compress">"good compression algorithm"</a>)
	  </ol>
  </section>

  <section>
	<h3>Big O notation recap</h3>
	Chapters 2.1 and 2.2 in Roughgarden's Algorithms Illuminated Part 1.
  </section>

  <section>
	<img data-src="images/rai_bigo_seven_words.png">
  </section>

  <section>
	<img data-src="images/rai_bigo_english_def.png">
  </section>

  <section>
	<img data-src="images/rai_bigo_picture_def.png">
  </section>

  <section>
	<img data-src="images/rai_bigo_math_def.png">
  </section>

  <section>
	<img data-src="images/rai_search_one_array.png">
  </section>

  <section>
	<img data-src="images/rai_search_two_arrays.png">
  </section>

  <section>
	<img data-src="images/rai_check_common_element.png">
  </section>
</section>

<section>
	<h3>Conditional Kolmogorov complexity: connection to pre-training</h3>
	$K(x|y)$ is the length of the smallest TM that generates $x$ given $y$.

	<p class="fragment" data-fragment-index="1">
	  <b>Properties:</b></br>
	  <ul class="fragment" data-fragment-index="1">
		<li> $K(x|\varnothing) = K(x)$
		<li> $K(x|x) = c$
	  <ul>
	</p>
</section>

<section data-markdown id="L7">
	## Lecture 7: Basic algorithms recap
</section>

<section data-markdown>
	### Why Study Algorithms?
	- Important for all other branches of computer science
	  - Routing protocols in communication networks piggyback on classical shortest path algorithms
	  - Public-key cryptography relies on efficient number-theoretic algorithms
	  - Computer graphics requires the computational primitives supplied by geometric algorithms
	  - Database indices rely on balanced search tree data structures
	  - Computational biology uses dynamic programming algorithms to measure genome similarity
</section>

<section data-markdown>
	### Why Study Algorithms?
	- Driver of technological innovation
	  - $\mathtt{PageRank}$
	  - From the President’s council of advisers on science and technology report to the United States White House, December 2010: *"Everyone knows Moore’s Law &mdash; a prediction made in 1965 by Intel co-founder Gordon Moore that the density of transistors in integrated circuits would continue to double every 1 to 2 years [...] in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed."*
</section>

<section data-markdown>
	### Why Study Algorithms?
	- Lens on other sciences
	- Good for the brain
	- Fun!
</section>

<section data-markdown>
	### Sorting
	#### Standard algorithms
	- $\mathtt{MergeSort}$
	- $\mathtt{QuickSort}$
	- [Sorting Networks](https://paperpile.com/app/p/c4f00e6c-9658-0c59-8301-c2fd300b9584)
	</ul>
</section>

<section>
	<h3>$\mathtt{MergeSort}$</h3>
	<img data-src="images/rai_mergesort_figure.png" width="750">
</section>

<section>
  <section>
	<h3>Running time of $\mathtt{MergeSort}$</h3>
	<b>Theorem 1.2</b> <i>For every input array of length $n \geq 1$, the $\mathtt{MergeSort}$ algorithm performs at most</i>
	$$
	  6n\log_2 n + 6n
	$$
	<i>operations, where $log_2$ denotes the base-2 logarithm.</i>
  </section>

  <section>
	<img data-src="images/rai_mergesort_code.png">
  </section>

  <section id="merge">
	<img data-src="images/rai_merge_subroutine.png">
  </section>

  <section>
	<img data-src="images/rai_mergesort_main_proof.png">
  </section>
</section>

<section data-markdown>
	### What is a "fast" algorithm?

	**Classically:** *A "fast algorithm" is an algorithm whose worst-case running time grows slowly with the input size.* [Tim Roughgarden]

	Possible alternative: *We define an algorithm to be any function that can be expressed with a short
program.* [Wojciech Zaremba]

	Either way, near-linear algorithms are "for-free primitives" that we can just run.
</section>

<section data-markdown id="L8">
	## Lecture 8: Sorting
</section>

<section>
	<h3>The divide-and-conquer paradigm</h3>
	<ol>
	  <li> <i>Divide</i> the input int smaller subproblems
	  <li> <i>Conquer</i> the subproblems recursively
	  <li> <i>Combine</i> the solution for the subproblems into a solution for the original problem
	</ol>

	Often, the clever bit is the <i>combine</i> step (e.g. the $\mathtt{Merge}$ subroutine in $\mathtt{MergeSort}$), but in $\mathtt{QuickSort}$, for example, the clever step is <i>divide</i>.
</section>

<section data-markdown>
	#### **Problem: Counting Inversions**

	- **Input:** An array $A$ of distinct integers.

	- **Output:** The number of inversions of $A$ &mdash; the number of pairs $(i, j)$ of array indices with $i < j$ and $A[i] > A[j]$.

	What is the largest possible number of inversions a 7-element array can have?
</section>

<section>
	$\mathtt{Merge}$-$\mathtt{and}$-$\mathtt{CountSplitInv}$<br><br>

	<img class="fragment" data-fragment-index="1" data-src="images/rai_count_split_inv.png" width="500">

	<p class="fragment" data-fragment-index="1">
	  Compare with <a href="#/merge">$\mathtt{Merge}$</a>
	</p>
</section>

<section data-markdown>
	### $\mathtt{QuickSort}$

	One of CS Greatest Hits

	$\mathtt{QuickSort}$ was invented by Tony Hoare, in 1959, when he was 25.
	Hoare was awarded the ACM Turing Award — CS equivalent of the Nobel Prize in — 1980.

	![](images/rai_pivot_example.png)
</section>

<section>
	<img data-src="images/rai_quicksort_high_level.png" width="750">
</section>

<section data-markdown>
	Partition subroutine easy way

	![](images/rai_quicksort_partition_example.png)

	Complexity? Memory?
</section>

<section data-markdown>
	![](images/rai_quicksort_partition_overview.svg)
</section>

<section>
	<img data-src="images/rai_quicksort_main_example.svg" width="725">
</section>

<section data-markdown>
	![](images/rai_quicksort_partition_pseudocode.png)
</section>

<section data-markdown>
	![](images/rai_quicksort_pseudocode.png)
</section>

<section id="LX">
	<h2>Lecture X: <a href="https://youtu.be/733m6qBH-jI?t=57">How to read papers</a></h2>
</section>


</div>
</div>

<script src="../talks/revealjs/dist/reveal.js"></script>
<script src="../talks/revealjs/plugin/markdown/markdown.js"></script>
<script src="../talks/revealjs/plugin/highlight/highlight.js"></script>
<script src="../talks/revealjs/plugin/notes/notes.js"></script>
<script src="../talks/revealjs/plugin/math/math.js"></script>
<script>
	Reveal.initialize({
		controls: true,
		progress: true,
		history: true,
		center: true,
		slideNumber: 'h.v',
		plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.MathJax2 ]
			});
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86783531-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
